./logger.txt

start time: 2023_06_23_20_35_09
Namespace(alph=[1.0, 50.0, 1.0], batch_size=2000, drop_freq=0, early_stopping=20, evaluate=False, f=None, gpu=1, log_freq=10, lr=0.001, lr_drop=10.0, m=256, nTh=2, niters=8000, nt=8, nt_val=16, prec='single', resume=None, save='experiments/cnf/large', test_batch_size=2000, weight_decay=0.0)
./logger.txt

start time: 2023_06_23_20_36_15
Namespace(alph=[1.0, 50.0, 1.0], batch_size=2000, drop_freq=0, early_stopping=20, evaluate=False, f=None, gpu=1, log_freq=10, lr=0.001, lr_drop=10.0, m=256, nTh=2, niters=8000, nt=8, nt_val=16, prec='single', resume=None, save='experiments/cnf/large', test_batch_size=2000, weight_decay=0.0)
./logger.txt

start time: 2023_06_23_20_37_08
Namespace(alph=[1.0, 50.0, 1.0], batch_size=2000, drop_freq=0, early_stopping=20, evaluate=False, f=None, gpu=1, log_freq=10, lr=0.001, lr_drop=10.0, m=256, nTh=2, niters=8000, nt=8, nt_val=16, prec='single', resume=None, save='experiments/cnf/large', test_batch_size=2000, weight_decay=0.0)
./logger.txt

start time: 2023_06_23_20_37_48
Namespace(alph=[1.0, 50.0, 1.0], batch_size=2000, drop_freq=0, early_stopping=20, evaluate=False, f=None, gpu=1, log_freq=10, lr=0.001, lr_drop=10.0, m=256, nTh=2, niters=8000, nt=8, nt_val=16, prec='single', resume=None, save='experiments/cnf/large', test_batch_size=2000, weight_decay=0.0)
Phi(
  (c): Linear(in_features=44, out_features=1, bias=True)
  (w): Linear(in_features=256, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=44, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
)
-------------------------
DIMENSION=43  m=256  nTh=2   alpha=[1.0, 50.0, 1.0]
nt=8   nt_val=16
Number of trainable parameters: 78053
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
data=tensor([[-1.7440, -0.6487, -0.4123,  ..., -1.0927, -0.6411,  0.1727],
        [ 0.0186, -0.4162, -0.4202,  ..., -0.6987, -0.3812, -0.1691],
        [-0.6450,  0.3083, -0.3648,  ..., -0.6345, -0.4587, -1.2745],
        ...,
        [ 0.1635, -0.4155,  0.4509,  ..., -0.6174,  0.3620,  1.2194],
        [-0.1786, -0.8769, -0.3307,  ...,  0.1630, -0.9731,  0.6313],
        [ 0.4658, -0.5431,  0.6389,  ..., -0.0230, -0.3859,  0.3635]]) batch_size=2000 gpu=1
maxIters=8000 
saveLocation = experiments/cnf/large
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)        valLoss    valL       valC       valR       
./logger.txt

start time: 2023_06_23_20_39_35
Namespace(alph=[1.0, 50.0, 1.0], batch_size=2000, drop_freq=0, early_stopping=20, evaluate=False, f=None, gpu=1, log_freq=10, lr=0.001, lr_drop=10.0, m=256, nTh=2, niters=8000, nt=8, nt_val=16, prec='single', resume=None, save='experiments/cnf/large', test_batch_size=2000, weight_decay=0.0)
Phi(
  (c): Linear(in_features=44, out_features=1, bias=True)
  (w): Linear(in_features=256, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=44, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
)
-------------------------
DIMENSION=43  m=256  nTh=2   alpha=[1.0, 50.0, 1.0]
nt=8   nt_val=16
Number of trainable parameters: 78053
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
data=tensor([[-1.7440, -0.6487, -0.4123,  ..., -1.0927, -0.6411,  0.1727],
        [ 0.0186, -0.4162, -0.4202,  ..., -0.6987, -0.3812, -0.1691],
        [-0.6450,  0.3083, -0.3648,  ..., -0.6345, -0.4587, -1.2745],
        ...,
        [ 0.1635, -0.4155,  0.4509,  ..., -0.6174,  0.3620,  1.2194],
        [-0.1786, -0.8769, -0.3307,  ...,  0.1630, -0.9731,  0.6313],
        [ 0.4658, -0.5431,  0.6389,  ..., -0.0230, -0.3859,  0.3635]]) batch_size=2000 gpu=1
maxIters=8000 
saveLocation = experiments/cnf/large
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)        valLoss    valL       valC       valR       
./logger.txt

start time: 2023_06_23_20_40_24
Namespace(alph=[1.0, 50.0, 1.0], batch_size=2000, drop_freq=0, early_stopping=20, evaluate=False, f=None, gpu=1, log_freq=10, lr=0.001, lr_drop=10.0, m=256, nTh=2, niters=8000, nt=8, nt_val=16, prec='single', resume=None, save='experiments/cnf/large', test_batch_size=2000, weight_decay=0.0)
Phi(
  (c): Linear(in_features=44, out_features=1, bias=True)
  (w): Linear(in_features=256, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=44, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
)
-------------------------
DIMENSION=43  m=256  nTh=2   alpha=[1.0, 50.0, 1.0]
nt=8   nt_val=16
Number of trainable parameters: 78053
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
data=tensor([[-1.7440, -0.6487, -0.4123,  ..., -1.0927, -0.6411,  0.1727],
        [ 0.0186, -0.4162, -0.4202,  ..., -0.6987, -0.3812, -0.1691],
        [-0.6450,  0.3083, -0.3648,  ..., -0.6345, -0.4587, -1.2745],
        ...,
        [ 0.1635, -0.4155,  0.4509,  ..., -0.6174,  0.3620,  1.2194],
        [-0.1786, -0.8769, -0.3307,  ...,  0.1630, -0.9731,  0.6313],
        [ 0.4658, -0.5431,  0.6389,  ..., -0.0230, -0.3859,  0.3635]]) batch_size=2000 gpu=1
maxIters=8000 
saveLocation = experiments/cnf/large
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)        valLoss    valL       valC       valR       
./logger.txt

start time: 2023_06_23_20_48_10
Namespace(alph=[1.0, 50.0, 1.0], batch_size=2000, drop_freq=0, early_stopping=20, evaluate=False, f=None, gpu=1, log_freq=10, lr=0.001, lr_drop=10.0, m=256, nTh=2, niters=8000, nt=8, nt_val=16, prec='single', resume=None, save='experiments/cnf/large', test_batch_size=2000, weight_decay=0.0)
Phi(
  (c): Linear(in_features=44, out_features=1, bias=True)
  (w): Linear(in_features=256, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=44, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
)
-------------------------
DIMENSION=43  m=256  nTh=2   alpha=[1.0, 50.0, 1.0]
nt=8   nt_val=16
Number of trainable parameters: 78053
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
data=tensor([[-1.7440, -0.6487, -0.4123,  ..., -1.0927, -0.6411,  0.1727],
        [ 0.0186, -0.4162, -0.4202,  ..., -0.6987, -0.3812, -0.1691],
        [-0.6450,  0.3083, -0.3648,  ..., -0.6345, -0.4587, -1.2745],
        ...,
        [ 0.1635, -0.4155,  0.4509,  ..., -0.6174,  0.3620,  1.2194],
        [-0.1786, -0.8769, -0.3307,  ...,  0.1630, -0.9731,  0.6313],
        [ 0.4658, -0.5431,  0.6389,  ..., -0.0230, -0.3859,  0.3635]]) batch_size=2000 gpu=1
maxIters=8000 
saveLocation = experiments/cnf/large
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)        valLoss    valL       valC       valR       
./logger.txt

start time: 2023_06_23_20_49_02
Namespace(alph=[1.0, 50.0, 1.0], batch_size=2000, drop_freq=0, early_stopping=20, evaluate=False, f=None, gpu=1, log_freq=10, lr=0.001, lr_drop=10.0, m=256, nTh=2, niters=8000, nt=8, nt_val=16, prec='single', resume=None, save='experiments/cnf/large', test_batch_size=2000, weight_decay=0.0)
Phi(
  (c): Linear(in_features=44, out_features=1, bias=True)
  (w): Linear(in_features=256, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=44, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
)
-------------------------
DIMENSION=43  m=256  nTh=2   alpha=[1.0, 50.0, 1.0]
nt=8   nt_val=16
Number of trainable parameters: 78053
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
data=tensor([[-1.7440, -0.6487, -0.4123,  ..., -1.0927, -0.6411,  0.1727],
        [ 0.0186, -0.4162, -0.4202,  ..., -0.6987, -0.3812, -0.1691],
        [-0.6450,  0.3083, -0.3648,  ..., -0.6345, -0.4587, -1.2745],
        ...,
        [ 0.1635, -0.4155,  0.4509,  ..., -0.6174,  0.3620,  1.2194],
        [-0.1786, -0.8769, -0.3307,  ...,  0.1630, -0.9731,  0.6313],
        [ 0.4658, -0.5431,  0.6389,  ..., -0.0230, -0.3859,  0.3635]]) batch_size=2000 gpu=1
maxIters=8000 
saveLocation = experiments/cnf/large
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)        valLoss    valL       valC       valR       
./logger.txt

start time: 2023_06_23_20_51_11
Namespace(alph=[1.0, 50.0, 1.0], batch_size=2000, drop_freq=0, early_stopping=20, evaluate=False, f=None, gpu=1, log_freq=10, lr=0.001, lr_drop=10.0, m=256, nTh=2, niters=8000, nt=8, nt_val=16, prec='single', resume=None, save='experiments/cnf/large', test_batch_size=2000, weight_decay=0.0)
Phi(
  (c): Linear(in_features=44, out_features=1, bias=True)
  (w): Linear(in_features=256, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=44, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
)
-------------------------
DIMENSION=43  m=256  nTh=2   alpha=[1.0, 50.0, 1.0]
nt=8   nt_val=16
Number of trainable parameters: 78053
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
data=tensor([[-1.7440, -0.6487, -0.4123,  ..., -1.0927, -0.6411,  0.1727],
        [ 0.0186, -0.4162, -0.4202,  ..., -0.6987, -0.3812, -0.1691],
        [-0.6450,  0.3083, -0.3648,  ..., -0.6345, -0.4587, -1.2745],
        ...,
        [ 0.1635, -0.4155,  0.4509,  ..., -0.6174,  0.3620,  1.2194],
        [-0.1786, -0.8769, -0.3307,  ...,  0.1630, -0.9731,  0.6313],
        [ 0.4658, -0.5431,  0.6389,  ..., -0.0230, -0.3859,  0.3635]]) batch_size=2000 gpu=1
maxIters=8000 
saveLocation = experiments/cnf/large
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)        valLoss    valL       valC       valR       
